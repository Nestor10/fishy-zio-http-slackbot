app {
  ping-interval-seconds = 5
  
  # LLM Configuration
  # Supports Ollama (local), OpenAI, Anthropic, or any OpenAI-compatible API
  llm {
    # Base URL for the LLM API
    # - Ollama (default): http://localhost:11434
    # - OpenAI: https://api.openai.com
    # - Azure OpenAI: https://<resource>.openai.azure.com
    # - Other providers: check their docs
    base-url = "http://localhost:11434"
    base-url = ${?APP_LLM_BASE_URL}
    
    # API Key for authentication (required for most cloud providers)
    # - Ollama: not required (omit or leave empty)
    # - OpenAI: sk-... (get from platform.openai.com)
    # - Anthropic: sk-ant-... (get from console.anthropic.com)
    # api-key = "sk-..."
    api-key = ${?APP_LLM_API_KEY}
    
    # Model identifier
    # - Ollama: qwen2.5:0.5b, llama2, mistral, etc.
    # - OpenAI: gpt-4, gpt-3.5-turbo, etc.
    # - Anthropic: claude-3-opus-20240229, etc.
    model = "qwen2.5:0.5b"
    model = ${?APP_LLM_MODEL}
    
    # Temperature (0.0 = deterministic, 1.0 = creative)
    temperature = 0.7
    temperature = ${?APP_LLM_TEMPERATURE}
    
    # Max tokens is optional - omit to let the model decide
    # max-tokens = 512
    # max-tokens = ${?APP_LLM_MAX_TOKENS}
    
    # System prompt (sets bot personality/behavior)
    system-prompt = "You are a helpful assistant in a Slack workspace. Be concise and friendly."
    system-prompt = ${?APP_LLM_SYSTEM_PROMPT}
  }
}

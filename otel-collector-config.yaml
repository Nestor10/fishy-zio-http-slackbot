# OpenTelemetry Collector Configuration
# This is the recommended production pattern - all services send to the collector,
# which then routes to appropriate backends (Jaeger, Prometheus, Loki, etc.)

receivers:
  # OTLP receiver - accepts traces, metrics, and logs from instrumented apps
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317  # Your app sends here
      http:
        endpoint: 0.0.0.0:4318  # Alternative HTTP endpoint

processors:
  # Batch processor - improves performance by batching telemetry data
  batch:
    timeout: 1s
    send_batch_size: 1024
    send_batch_max_size: 2048

  # Memory limiter - prevents OOM by dropping data when memory limit reached
  memory_limiter:
    check_interval: 1s
    limit_mib: 512

  # Resource detection - automatically adds host, container, k8s metadata
  resourcedetection:
    detectors: [env, system]
    timeout: 5s

  # Sampling - reduce volume in production (100% for dev)
  probabilistic_sampler:
    sampling_percentage: 100  # 100% for dev, reduce to 1-10% for prod

exporters:
  # Jaeger exporter - for distributed tracing
  otlp/jaeger:
    endpoint: jaeger:4317
    tls:
      insecure: true

  # Prometheus exporter - for metrics (exposes /metrics endpoint)
  prometheus:
    endpoint: 0.0.0.0:8889
    namespace: slackbot
    const_labels:
      service: slack-bot

  # Debug exporter - for debugging (prints to stdout)
  debug:
    verbosity: detailed
    sampling_initial: 5
    sampling_thereafter: 200

  # OTLP exporter - generic (can point to any OTLP-compatible backend)
  # Useful for: Grafana Cloud, DataDog, New Relic, etc.
  # otlp/external:
  #   endpoint: https://your-vendor.com:4317
  #   headers:
  #     api-key: ${OTLP_API_KEY}

service:
  pipelines:
    # Traces pipeline - routes traces to Jaeger
    traces:
      receivers: [otlp]
      processors: [memory_limiter, resourcedetection, probabilistic_sampler, batch]
      exporters: [otlp/jaeger, debug]

    # Metrics pipeline - routes metrics to Prometheus
    metrics:
      receivers: [otlp]
      processors: [memory_limiter, resourcedetection, batch]
      exporters: [prometheus, debug]

    # Logs pipeline - future (when you add log correlation)
    # logs:
    #   receivers: [otlp]
    #   processors: [memory_limiter, resourcedetection, batch]
    #   exporters: [logging]

  # Telemetry - collector's own metrics
  telemetry:
    logs:
      level: info
    metrics:
      level: detailed
